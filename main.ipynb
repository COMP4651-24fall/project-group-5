{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Pre-processing'''\n",
    "data_path = '../data/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv')\n",
    "test = pd.read_csv(data_path + 'test.csv')\n",
    "\n",
    "songs = pd.read_csv(data_path + 'songs.csv')\n",
    "\n",
    "members = pd.read_csv(data_path + 'members.csv', parse_dates=['registration_init_time','expiration_date'])\n",
    "\n",
    "train = train.merge(songs, on='song_id', how='left')\n",
    "\n",
    "songs_extra = pd.read_csv(data_path + 'song_extra_info.csv')\n",
    "\n",
    "test = test.merge(songs, on='song_id', how='left')\n",
    "y_all = train['target'].values\n",
    "\n",
    "members['membership_days'] = members['expiration_date'].subtract(members['registration_init_time']).dt.days.astype(int)\n",
    "\n",
    "members['registration_year'] = members['registration_init_time'].dt.year\n",
    "members['registration_month'] = members['registration_init_time'].dt.month\n",
    "members['registration_date'] = members['registration_init_time'].dt.day\n",
    "\n",
    "members['expiration_year'] = members['expiration_date'].dt.year\n",
    "members['expiration_month'] = members['expiration_date'].dt.month\n",
    "members['expiration_date'] = members['expiration_date'].dt.day\n",
    "members = members.drop(['registration_init_time'], axis=1)\n",
    "\n",
    "train = train.merge(members, on='msno', how='left')\n",
    "test = test.merge(members, on='msno', how = 'left')\n",
    "\n",
    "# print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Pre-processing (Cont'd)'''\n",
    "def isrc_to_year(isrc):\n",
    "    if type(isrc) == str:\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan\n",
    "        \n",
    "songs_extra['song_year'] = songs_extra['isrc'].apply(isrc_to_year)\n",
    "songs_extra.drop(['isrc', 'name'], axis = 1, inplace = True)\n",
    "\n",
    "train = train.merge(songs_extra, on = 'song_id', how = 'left')\n",
    "test = test.merge(songs_extra, on = 'song_id', how = 'left')\n",
    "\n",
    "def genre_id_count(x):\n",
    "    if x == 'no_genre_id':\n",
    "        return 0\n",
    "    else:\n",
    "        return x.count('|') + 1\n",
    "        \n",
    "train['genre_ids'].fillna('no_genre_id',inplace=True)\n",
    "test['genre_ids'].fillna('no_genre_id',inplace=True)\n",
    "train['genre_ids_count'] = train['genre_ids'].apply(genre_id_count).astype(np.int8)\n",
    "test['genre_ids_count'] = test['genre_ids'].apply(genre_id_count).astype(np.int8)\n",
    "\n",
    "def lyricist_count(x):\n",
    "    if x == 'no_lyricist':\n",
    "        return 0\n",
    "    else:\n",
    "        return sum(map(x.count, ['|', '/', '\\\\', ';'])) + 1\n",
    "    return sum(map(x.count, ['|', '/', '\\\\', ';']))\n",
    "\n",
    "train['lyricist'].fillna('no_lyricist',inplace=True)\n",
    "test['lyricist'].fillna('no_lyricist',inplace=True)\n",
    "train['lyricists_count'] = train['lyricist'].apply(lyricist_count).astype(np.int8)\n",
    "test['lyricists_count'] = test['lyricist'].apply(lyricist_count).astype(np.int8)\n",
    "\n",
    "def composer_count(x):\n",
    "    if x == 'no_composer':\n",
    "        return 0\n",
    "    else:\n",
    "        return sum(map(x.count, ['|', '/', '\\\\', ';'])) + 1\n",
    "\n",
    "train['composer'].fillna('no_composer',inplace=True)\n",
    "test['composer'].fillna('no_composer',inplace=True)\n",
    "train['composer_count'] = train['composer'].apply(composer_count).astype(np.int8)\n",
    "test['composer_count'] = test['composer'].apply(composer_count).astype(np.int8)\n",
    "\n",
    "def is_featured(x):\n",
    "    if 'feat' in str(x) :\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "train['artist_name'].fillna('no_artist',inplace=True)\n",
    "test['artist_name'].fillna('no_artist',inplace=True)\n",
    "train['is_featured'] = train['artist_name'].apply(is_featured).astype(np.int8)\n",
    "test['is_featured'] = test['artist_name'].apply(is_featured).astype(np.int8)\n",
    "\n",
    "def artist_count(x):\n",
    "    if x == 'no_artist':\n",
    "        return 0\n",
    "    else:\n",
    "        return x.count('and') + x.count(',') + x.count('feat') + x.count('&')\n",
    "\n",
    "train['artist_count'] = train['artist_name'].apply(artist_count).astype(np.int8)\n",
    "test['artist_count'] = test['artist_name'].apply(artist_count).astype(np.int8)\n",
    "\n",
    "# if artist is same as composer\n",
    "train['artist_composer'] = (train['artist_name'] == train['composer']).astype(np.int8)\n",
    "test['artist_composer'] = (test['artist_name'] == test['composer']).astype(np.int8)\n",
    "\n",
    "# if artist, lyricist and composer are all three same\n",
    "train['artist_composer_lyricist'] = ((train['artist_name'] == train['composer']) & (train['artist_name'] == train['lyricist']) & (train['composer'] == train['lyricist'])).astype(np.int8)\n",
    "test['artist_composer_lyricist'] = ((test['artist_name'] == test['composer']) & (test['artist_name'] == test['lyricist']) & (test['composer'] == test['lyricist'])).astype(np.int8)\n",
    "\n",
    "_mean_song_length = np.mean(train['song_length'])\n",
    "# def smaller_song(x):\n",
    "    # if x < _mean_song_length:\n",
    "    #     return 1\n",
    "    # return 0\n",
    "\n",
    "# train['smaller_song'] = train['song_length'].apply(smaller_song).astype(np.int8)\n",
    "# test['smaller_song'] = test['song_length'].apply(smaller_song).astype(np.int8)\n",
    "\n",
    "# # number of times a song has been played before\n",
    "# _dict_count_song_played_train = {k: v for k, v in train['song_id'].value_counts().iteritems()}\n",
    "# _dict_count_song_played_test = {k: v for k, v in test['song_id'].value_counts().iteritems()}\n",
    "# def count_song_played(x):\n",
    "    try:\n",
    "        return _dict_count_song_played_train[x]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            return _dict_count_song_played_test[x]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "# train['count_song_played'] = train['song_id'].apply(count_song_played).astype(np.int64)\n",
    "# test['count_song_played'] = test['song_id'].apply(count_song_played).astype(np.int64)\n",
    "\n",
    "# # number of times the artist has been played\n",
    "# _dict_count_artist_played_train = {k: v for k, v in train['artist_name'].value_counts().iteritems()}\n",
    "# _dict_count_artist_played_test = {k: v for k, v in test['artist_name'].value_counts().iteritems()}\n",
    "# def count_artist_played(x):\n",
    "    try:\n",
    "        return _dict_count_artist_played_train[x]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            return _dict_count_artist_played_test[x]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "# train['count_artist_played'] = train['artist_name'].apply(count_artist_played).astype(np.int64)\n",
    "# test['count_artist_played'] = test['artist_name'].apply(count_artist_played).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 114. GiB for an array with shape (1048575, 116764) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m'''LSTM Model'''\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m      3\u001b[0m train_Y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m      4\u001b[0m dev_X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(pd\u001b[38;5;241m.\u001b[39mget_dummies(test\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mTensor)\n",
      "File \u001b[1;32mc:\\Users\\TheWind\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\encoding.py:214\u001b[0m, in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    210\u001b[0m     with_dummies \u001b[38;5;241m=\u001b[39m [data\u001b[38;5;241m.\u001b[39mselect_dtypes(exclude\u001b[38;5;241m=\u001b[39mdtypes_to_encode)]\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, pre, sep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data_to_encode\u001b[38;5;241m.\u001b[39mitems(), prefix, prefix_sep):\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# col is (column_name, column), use just column data here\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     dummy \u001b[38;5;241m=\u001b[39m \u001b[43m_get_dummies_1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix_sep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdummy_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m     with_dummies\u001b[38;5;241m.\u001b[39mappend(dummy)\n\u001b[0;32m    224\u001b[0m result \u001b[38;5;241m=\u001b[39m concat(with_dummies, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\TheWind\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\encoding.py:353\u001b[0m, in \u001b[0;36m_get_dummies_1d\u001b[1;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     dummy_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_\n\u001b[1;32m--> 353\u001b[0m dummy_mat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m dummy_mat[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(codes)), codes] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dummy_na:\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# reset NaN GH4446\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 114. GiB for an array with shape (1048575, 116764) and data type bool"
     ]
    }
   ],
   "source": [
    "'''LSTM Model'''\n",
    "train_X = torch.from_numpy(pd.get_dummies(train.drop(['target'], axis=1)).values).type(torch.Tensor)\n",
    "train_Y = torch.from_numpy(train['target'].values).type(torch.Tensor)\n",
    "dev_X = torch.from_numpy(pd.get_dummies(test.drop(['id'], axis=1)).values).type(torch.Tensor)\n",
    "dev_Y = torch.from_numpy(test['id'].values).type(torch.Tensor)\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "# all training data (epoch) / batch_size == num_batches (12)\n",
    "num_batches = int(train_X.shape[0] / batch_size)\n",
    "num_dev_batches = int(dev_X.shape[0] / batch_size)\n",
    "\n",
    "val_loss_list, val_accuracy_list, epoch_list = [], [], []\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=8, num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # setup LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        # setup output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        # lstm step => then ONLY take the sequence's final timetep to pass into the linear/dense layer\n",
    "        # Note: lstm_out contains outputs for every step of the sequence we are looping over (for BPTT)\n",
    "        # but we just need the output of the last step of the sequence, aka lstm_out[-1]\n",
    "        lstm_out, hidden = self.lstm(input, hidden)\n",
    "        logits = self.linear(lstm_out[-1])              # equivalent to return_sequences=False from Keras\n",
    "        genre_scores = F.log_softmax(logits, dim=1)\n",
    "        return genre_scores, hidden\n",
    "\n",
    "    def get_accuracy(self, logits, target):\n",
    "        \"\"\" compute accuracy for training round \"\"\"\n",
    "        corrects = (\n",
    "                torch.max(logits, 1)[1].view(target.size()).data == target.data\n",
    "        ).sum()\n",
    "        accuracy = 100.0 * corrects / self.batch_size\n",
    "        return accuracy.item()\n",
    "\n",
    "def main(): # WIP\n",
    "    model = LSTM(\n",
    "        input_dim=33, hidden_dim=128, batch_size=batch_size, output_dim=8, num_layers=2\n",
    "    )\n",
    "    loss_function = nn.NLLLoss()  # expects ouputs from LogSoftmax\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # To keep LSTM stateful between batches, you can set stateful = True, which is not suggested for training\n",
    "    stateful = False\n",
    "\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "    if train_on_gpu:\n",
    "        print(\"\\nTraining on CUDA GPU\")\n",
    "    else:\n",
    "        print(\"\\nTraining on CPU (CUDA GPU not found)\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_running_loss, train_acc = 0.0, 0.0\n",
    "\n",
    "        # Init hidden state - if you don't want a stateful LSTM (between epochs)\n",
    "        hidden_state = None\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            # zero out gradient, so they don't accumulate btw batches\n",
    "            model.zero_grad()\n",
    "\n",
    "            # train_X shape: (total # of training examples, sequence_length, input_dim)\n",
    "            # train_Y shape: (total # of training examples, # output classes)\n",
    "            #\n",
    "            # Slice out local minibatches & labels => Note that we *permute* the local minibatch to\n",
    "            # match the PyTorch expected input tensor format of (sequence_length, batch size, input_dim)\n",
    "            X_local_minibatch, y_local_minibatch = (\n",
    "                train_X[i * batch_size: (i + 1) * batch_size, ],\n",
    "                train_Y[i * batch_size: (i + 1) * batch_size, ],\n",
    "            )\n",
    "\n",
    "            # Reshape input & targets to \"match\" what the loss_function wants\n",
    "            X_local_minibatch = X_local_minibatch.permute(1, 0, 2)\n",
    "\n",
    "            # NLLLoss does not expect a one-hot encoded vector as the target, but class indices\n",
    "            y_local_minibatch = torch.max(y_local_minibatch, 1)[1]\n",
    "\n",
    "            y_pred, hidden_state = model(X_local_minibatch, hidden_state)  # forward pass\n",
    "\n",
    "            # Stateful = False for training. Do we go Stateful = True during inference/prediction time?\n",
    "            if not stateful:\n",
    "                hidden_state = None\n",
    "            else:\n",
    "                h_0, c_0 = hidden_state\n",
    "                h_0.detach_(), c_0.detach_()\n",
    "                hidden_state = (h_0, c_0)\n",
    "\n",
    "            loss = loss_function(y_pred, y_local_minibatch)  # compute loss\n",
    "            loss.backward()  # backward pass\n",
    "            optimizer.step()  # parameter update\n",
    "\n",
    "            train_running_loss += loss.detach().item()  # unpacks the tensor into a scalar value\n",
    "            train_acc += model.get_accuracy(y_pred, y_local_minibatch)\n",
    "\n",
    "        print(\n",
    "            \"Epoch:  %d | NLLoss: %.4f | Train Accuracy: %.2f\"\n",
    "            % (epoch, train_running_loss / num_batches, train_acc / num_batches)\n",
    "        )\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Validation ...\")  # should this be done every N=10 epochs\n",
    "            val_running_loss, val_acc = 0.0, 0.0\n",
    "\n",
    "            # Compute validation loss, accuracy. Use torch.no_grad() & model.eval()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "\n",
    "                hidden_state = None\n",
    "                for i in range(num_dev_batches):\n",
    "                    X_local_validation_minibatch, y_local_validation_minibatch = (\n",
    "                        dev_X[i * batch_size: (i + 1) * batch_size, ],\n",
    "                        dev_Y[i * batch_size: (i + 1) * batch_size, ],\n",
    "                    )\n",
    "                    X_local_minibatch = X_local_validation_minibatch.permute(1, 0, 2)\n",
    "                    y_local_minibatch = torch.max(y_local_validation_minibatch, 1)[1]\n",
    "\n",
    "                    y_pred, hidden_state = model(X_local_minibatch, hidden_state)\n",
    "                    if not stateful:\n",
    "                        hidden_state = None\n",
    "\n",
    "                    val_loss = loss_function(y_pred, y_local_minibatch)\n",
    "\n",
    "                    val_running_loss += (\n",
    "                        val_loss.detach().item()\n",
    "                    )  # unpacks the tensor into a scalar value\n",
    "                    val_acc += model.get_accuracy(y_pred, y_local_minibatch)\n",
    "\n",
    "                model.train()  # reset to train mode after iterationg through validation data\n",
    "                print(\n",
    "                    \"Epoch:  %d | NLLoss: %.4f | Train Accuracy: %.2f | Val Loss %.4f  | Val Accuracy: %.2f\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        train_running_loss / num_batches,\n",
    "                        train_acc / num_batches,\n",
    "                        val_running_loss / num_dev_batches,\n",
    "                        val_acc / num_dev_batches,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            epoch_list.append(epoch)\n",
    "            val_accuracy_list.append(val_acc / num_dev_batches)\n",
    "            val_loss_list.append(val_running_loss / num_dev_batches)\n",
    "\n",
    "    # visualization loss\n",
    "    plt.plot(epoch_list, val_loss_list)\n",
    "    plt.xlabel(\"# of epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"LSTM: Loss vs # epochs\")\n",
    "    plt.show()\n",
    "\n",
    "    # visualization accuracy\n",
    "    plt.plot(epoch_list, val_accuracy_list, color=\"red\")\n",
    "    plt.xlabel(\"# of epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"LSTM: Accuracy vs # epochs\")\n",
    "    # plt.savefig('graph.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
